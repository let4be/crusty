#First of all let's settle on some widely used terminology - remember this, it's widely used everywhere ;)
#	JOB(=domain, =set of TASKs)
#	TASK(=page, =part of JOB)
#
#
#

log:
  # fancy some colors? - disable if redirect to file is intended
  ansi: false
  # base-line log level
  level: info
  # my_crate=info,my_crate::my_mod=debug,[my_span]=trace
  # see https://tracing.rs/tracing_subscriber/filter/struct.envfilter
  #"[task{name=Crusty::go}]=info", "[task{name=Crusty::job_reader}]=info"
  #filter: ["[task{name=TaskProcessor::go}]=warn", "[task{name=TaskScheduler::go}]=warn", "[task{name=Crawler::go}]=warn"]
host: crawler-1 # for metrics

# as soon as we see SIGTERM we stop consuming new jobs from redis queue and switch to one of shutdown states
# graceful - we have time to spare
#     1) start counting graceful_timeout
#     2) WAIT for second SIGTERM or graceful_timeout elapsed - switch to immediate shutdown mode
#     3) WAIT if we processed all jobs and persisted everything to db THEN exit normally
# immediate - we need to be exit asap
#     1) stop all crawling activity no matter the state of it
#     2) WAIT if we persisted everything to db THEN exit normally
shutdown:
  # putting graceful_timeout to 0 basically means we go straight to immediate shutdown mode
  graceful_timeout: 5s

# Redis settings (queue)
queue:
  redis:
    hosts: ["redis://172.16.2.1:6379/", "redis://172.16.2.2:6379/"]
  jobs:
    # min shard number we have access to(so called scoped shards)
    shard_min: 0
    # max shard number we have access to(so called scoped shards), effective scoped shards - 0, 1
    shard_max: 2
    # total number of all shards, in a multi-node setup shard_total >= shard_max - shard_min
    shard_total: 2

    # scoped shards - shards we dequeue jobs from and update job status at
    # total shards - shards we insert into, this is always 0..shard_total

    #we resolve IP of all discovered domains and calculate addr_key
    #1. Take only ipv4
    #2. Sort
    #3. Take first IP and apply addr_key_mask masking
    #4. addr_key = addr_key | addr_key_4_mask;
    #we now use addr_key in shard calculation, we never select more than domain_top_n domains from a given addr_key
    #this ensures we are being polite to websites with different domains hosted on the same IP(or subnet, depending on addr_key_4_mask)
    addr_key_mask: 24 #read as /24 meaning first 24 bits are significant while last 8 are not(will be masked)
    enqueue:
      options:
        # each added jobs has a TTL, we don't want jobs handing in redis memory forever...
        # the nature of broad web crawling is - almost everything significant will be discovered, eventually
        # dropping job here and there is no biggie
        ttl: 10m
      driver: #those are relabuf settings, see https://github.com/let4be/relabuf
        soft_cap: 500       # release when we've buffered soft_cap or more
        hard_cap: 1000      # do not buffer more than hard_cap, slow down the producer after hard_cap is reached
        release_after: 1s   # release when release_after since last successful release(or start) has passed
    finish:
      options:
        ttl: 10m
        # initial in-history bloom filter capacity, keep this big enough.
        # Scaling of bloom filter is relatively expensive and will affect performance of all future computations against this filter
        bf_initial_capacity: 10000000
        # we won't check bf_error_rate of all discovered domains... it's the price for predictable memory footprint && efficiency
        bf_error_rate: 0.001
        # when filter is at capacity expand it by multiplying current capacity by this factor
        bf_expansion_factor: 2
      driver:
        soft_cap: 500
        hard_cap: 1000
        release_after: 1s
    dequeue:
      options:
        # up to limit jobs at once
        limit: 10000
        # dequeued jobs are being put in the in-processing list with TTL in case of sudden Crusty node explosion
        # we do NOT want jobs getting stuck "in processing" state forever!
        # keep this strictly > reader.default_crawling_settings job_hard_timeout
        ttl: 10m
        # do not emit job permits more frequently than
        emit_permit_delay: 1s
      # we reuse the same relabuf functionality when looking for new jobs
      # the basic concept is - we regularly emit so called job look permits, and the rest of relabuf rules apply(for releasing those permits)
      driver:
        # look for new jobs when internal buffer gets empty AND we have at least 1 outstanding permit
        soft_cap: 1
        # do not keep more than hard_cap permits(backoff permit producer)
        hard_cap: 2
        # look for new jobs also by time(if permit is available)
        release_after: 1s
    # select new jobs(domains) from a sharded queue stored in redis
    reader:
      # jobs are sharded, do not ask the same shard for job unless duration has passed since last time we asked
      shard_min_last_read: 1s
      # initial list of seed URLs to start the broad crawling from, additionally we also read seeds from CRUSTY_SEEDS env. variable
      seeds: [ ]

topk:
  redis:
    hosts: ["redis://172.16.3.1:6379/"]
  options:
    name: "top-domains"
    k: 100
    width: 25000
    depth: 8
    decay: 0.92
    consume_interval: 15s
  driver:
    soft_cap: 250
    hard_cap: 1000
    release_after: 1s

# Clickhouse database settings (metrics)
clickhouse:
  url: http://172.16.1.1:8123
  username: default
  password: ""
  database: crusty

  # We persist various queue metrics
  metrics_queue:
    table_name: metrics_queue
    label: ""
    # we always try to write in bulk, buffer up to max items before writing
    buffer_capacity: 1000
    # while we're waiting for buffer to fill wake once in a while to check for force_write_duration
    check_for_force_write_duration: 100ms
    # if force_write_duration elapsed since last write but we yet not filled buffer_capacity force the write anyway
    force_write_duration: 500ms
    concurrency: 3

  # We persist some db metrics for further analysis
  metrics_db:
    table_name: metrics_db
    label: ""
    buffer_capacity: 1000
    check_for_force_write_duration: 100ms
    force_write_duration: 500ms
    concurrency: 3

  # We persist metrics and various meta-data for each completed TASK(page)
  metrics_task:
    table_name: metrics_task
    label: ""
    buffer_capacity: 25000
    check_for_force_write_duration: 500ms
    force_write_duration: 5000ms
    concurrency: 3

  # We persist metrics and various meta-data for each finished JOB(set of pages => basically a domain)
  metrics_job:
    table_name: metrics_job
    label: ""
    buffer_capacity: 25000
    check_for_force_write_duration: 500ms
    force_write_duration: 5000ms
    concurrency: 3

  # We persist TOP K domains from redis into clickhouse(heavy hitters, glued on top of "TOP-K from redisbloom")
  topk:
    table_name: domain_topk
    label: ""
    buffer_capacity: 2500
    check_for_force_write_duration: 2500ms
    force_write_duration: 15000ms
    concurrency: 3

# crawling rules
rules:
  skip_no_follow_links: true
  # we limit N of links we can collect from a given JOB
  total_link_budget: 1000
  # we limit N of links we can collect from a given TASK
  links_per_task_budget: 100
  # do not go deeper than this level
  max_level: 25
  # should we respect robots.txt?
  robots_txt: true
  # after max redirects we stop trying to load a given page
  max_redirect: 5

# those settings relate to a crawler running on some particular domain
default_crawling_settings:
  # up to N pages concurrently, keep this number low to avoid excess stress
  concurrency: 2
  internal_read_buffer_size: 32kib
  max_response_size: 2mib
  # follow up to N redirects before giving up
  max_redirect: 5
  # 1s-5s is a safe bet to keep to avoid extra stress
  delay: 1s
  # vary delay time by this jitter(0..)
  delay_jitter: 1s
  # timeout for status loading(headers only)
  status_timeout: 5s
  # timeout for page loading and buffering(data)
  load_timeout: 10s
  # after soft timeout elapses we no longer queue new tasks for domain
  job_soft_timeout: 30s
  # after hard timeout elapses we forcibly stop the crawling job for this domain
  job_hard_timeout: 60s
  user_agent: "crusty/0.12.0"
  compression: true
  # custom headers are supported
  custom_headers:
    accept:
      - "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9"

# resolver settings
# leave empty for auto-conf
resolver:
  # number of concurrent green threads for name resolution(be mindful of your dns server capacity)
  # this should be configured carefully, low setting will lead to job starvation(inability to satisfy requested concurrency_profile.domain_concurrency)
  concurrency: 150

# domain discovery cache capacity, this cache helps to ease load on redis queue(so we do not try to insert billions of duplicated records)
# but because cache is local it's effectiveness will drop when adding new Crusty nodes
# but worry not, redis queue is so fast(and pre-sharded) we could run couple hundred crusty nodes with minimal additional setup
ddc_cap: 25000000
# recently discovered domains live in cache up to this duration
ddc_lifetime: 1h

# We monitor various internal queues and persist their status to db
queue_monitor_interval: 1s
# We parse HTML in a separate thread pool, stack size is configurable
# apparently even 32mib is not enough given max_response_size of 2mib...
parser_processor_stack_size: 128mib

# Fancy local address binding for monster setups with several NICs(local port limitation)
networking_profile:
  values:
    bind_local_ipv4: []
    bind_local_ipv6: []
    socket_read_buffer_size: 32kib
    socket_write_buffer_size: 32kib
    connect_timeout: 5s
  resolver_config:
    config:
      name_servers:
        - socket_addr: 172.16.0.254:53
          protocol: udp
    options:
      timeout: 5s
  #optionally blacklist some nets, reserved ipv4 are already blacklisted
  net_v4_blacklist: []

#leave commented for auto-conf
concurrency_profile:
  #= N of physical cores by default
  parser_concurrency: 8
  # pin N parser threads to core
  parser_pin: 0
  # We check multiple domains concurrency, set accordingly to saturate your hardware(cpu/network bound)
  domain_concurrency: 300
